{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment Analysis with TensorFlow\n",
    "\n",
    "A Convolutional Neural Net (CNN) is sometimes used in text classification tasks such as sentiment analysis.  We'll use a CNN built with TensorFlow to perform sentiment analysis in Amazon SageMaker on the IMDB dataset, which consists of movie reviews labeled as having positive or negative sentiment. Three aspects of Amazon SageMaker will be demonstrated:\n",
    "\n",
    "- How to use Script Mode with a prebuilt TensorFlow container, along with a training script similar to one you would use outside SageMaker. \n",
    "- Local Mode training, which allows you to test your code on your notebook instance before creating a full scale training job.\n",
    "- Batch Transform for offline, asynchronous predictions on large batches of data. \n",
    "\n",
    "#  Prepare Dataset\n",
    "\n",
    "We'll begin by loading the reviews dataset, and padding the reviews so all reviews have the same length.  Each review is represented as an array of numbers, where each number represents an indexed word.  Training data for both Local Mode and Hosted Training must be saved as files, so we'll also save the transformed data to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "csv_test_dir = os.path.join(os.getcwd(), 'data/csv-test')\n",
    "os.makedirs(csv_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "np.savetxt(os.path.join(csv_test_dir, 'csv-test.csv'), np.array(x_test[:100], dtype=np.int32), fmt='%d', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Mode Training\n",
    "\n",
    "Amazon SageMaker’s Local Mode training feature is a convenient way to make sure your code is working as expected before moving on to full scale, hosted training. With Local Mode, you can run quick tests with just a sample of training data, and/or a small number of epochs (passes over the full training set), while avoiding the time and expense of attempting full scale hosted training using possibly buggy code.  \n",
    "\n",
    "To train in Local Mode, it is necessary to have docker-compose or nvidia-docker-compose (for GPU) installed in the notebook instance. Running following script will install docker-compose or nvidia-docker-compose and configure the notebook environment for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ./setup.sh: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set up a TensorFlow Estimator for Local Mode training. A key parameters for the Estimator is the `train_instance_type`, which is the kind of hardware on which training will run. In the case of Local Mode, we simply set this parameter to `local_gpu` to invoke Local Mode training on the GPU, or to `local` if the instance has a CPU. Other parameters of note are the algorithm’s hyperparameters, which are passed in as a dictionary, and a Boolean parameter indicating that we are using Script Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 09:01:18.666472 4639487424 session.py:1106] Couldn't call 'get_role' to get Role ARN from role name olg to get Role path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The current AWS identity is not a role: arn:aws:iam::722321484884:user/olg, therefore it cannot be used as a SageMaker execution role",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9eea790de248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                        \u001b[0mtrain_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                        \u001b[0mhyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                        \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                        \u001b[0mbase_job_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf-keras-sentiment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                        \u001b[0mframework_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.13.1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'The current AWS identity is not a role: {}, therefore it cannot be used as a SageMaker execution role'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The current AWS identity is not a role: arn:aws:iam::722321484884:user/olg, therefore it cannot be used as a SageMaker execution role"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'local'\n",
    "tornasole_s3 = 's3://' + sagemaker.Session().default_bucket() + \"/tornasole-parameters/\"\n",
    "hyperparameters = {'epochs': 1, 'batch_size': 128, \n",
    "                   'tornasole-save-interval': 100, 'tornasole_outdir' : tornasole_s3 }\n",
    "local_estimator = TensorFlow(entry_point='sentiment_keras.py',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-keras-sentiment',\n",
    "                       framework_version='1.13.1',\n",
    "                       py_version='py3',\n",
    "                       image_name='072677473360.dkr.ecr.us-east-1.amazonaws.com/tornasole-preprod-tf-1.13.1-cpu:latest',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll briefly train the model in Local Mode.  Since this is just to make sure the code is working, we'll train for only one epoch.  (Note that on a CPU-based notebook instance, this one epoch will take at least 3 or 4 minutes.)  As you'll see from the logs below the cell when training is complete, even when trained for only one epoch, the accuracy of the model on training data is already at almost 80%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpsw39_nhj_algo-1-zwl3k_1 ... \n",
      "\u001b[1BAttaching to tmpsw39_nhj_algo-1-zwl3k_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:20:30,596 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:20:30,603 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:20:30,917 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:20:30,939 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:20:30,961 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:20:30,975 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"current_host\": \"algo-1-zwl3k\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"algo-1-zwl3k\"\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"batch_size\": 128,\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"tornasole-save-interval\": 100,\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"tornasole_outdir\": \"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"model_dir\": \"/opt/ml/model\"\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"job_name\": \"tf-keras-sentiment-2019-07-16-15-20-27-160\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"master_hostname\": \"algo-1-zwl3k\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-072677473360/tf-keras-sentiment-2019-07-16-15-20-27-160/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"module_name\": \"sentiment_keras\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"current_host\": \"algo-1-zwl3k\",\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m             \"algo-1-zwl3k\"\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m     \"user_entry_point\": \"sentiment_keras.py\"\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_HOSTS=[\"algo-1-zwl3k\"]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_HPS={\"batch_size\":128,\"epochs\":1,\"model_dir\":\"/opt/ml/model\",\"tornasole-save-interval\":100,\"tornasole_outdir\":\"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\"}\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_USER_ENTRY_POINT=sentiment_keras.py\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-zwl3k\",\"hosts\":[\"algo-1-zwl3k\"]}\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_CURRENT_HOST=algo-1-zwl3k\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_MODULE_NAME=sentiment_keras\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-072677473360/tf-keras-sentiment-2019-07-16-15-20-27-160/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-zwl3k\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-zwl3k\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":1,\"model_dir\":\"/opt/ml/model\",\"tornasole-save-interval\":100,\"tornasole_outdir\":\"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-keras-sentiment-2019-07-16-15-20-27-160\",\"log_level\":20,\"master_hostname\":\"algo-1-zwl3k\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-072677473360/tf-keras-sentiment-2019-07-16-15-20-27-160/source/sourcedir.tar.gz\",\"module_name\":\"sentiment_keras\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-zwl3k\",\"hosts\":[\"algo-1-zwl3k\"]},\"user_entry_point\":\"sentiment_keras.py\"}\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"1\",\"--model_dir\",\"/opt/ml/model\",\"--tornasole-save-interval\",\"100\",\"--tornasole_outdir\",\"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\"]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_HP_BATCH_SIZE=128\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_HP_TORNASOLE-SAVE-INTERVAL=100\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_HP_TORNASOLE_OUTDIR=s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m SM_HP_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m /usr/local/bin/python sentiment_keras.py --batch_size 128 --epochs 1 --model_dir /opt/ml/model --tornasole-save-interval 100 --tornasole_outdir s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Using TensorFlow backend.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m x train (25000, 400) y train (25000,)\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m [[  0   0   0 ...  19 178  32]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m  [  0   0   0 ...  16 145  95]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m  [  0   0   0 ...   7 129 113]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m  ...\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m  [595  13 258 ...  72  33  32]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m  [  0   0   0 ...  28 126 110]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m  [  0   0   0 ...   7  43  50]]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m [1 0 0 1 0 0 1 0 1 0]\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m x test (25000, 400) y test (25000,)\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Use tf.cast instead.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Use tf.cast instead.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Deprecated in favor of operator or tf.math.divide.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Deprecated in favor of operator or tf.math.divide.\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Train on 25000 samples, validate on 25000 samples\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m Epoch 1/1\n",
      "25000/25000 [==============================] - 390s 16ms/step - loss: 0.4266 - acc: 0.7852 - val_loss: 0.2631 - val_acc: 0.8911\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:27:04,144 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container.The model artifact was not saved in the TensorFlow SavedModel directory structure:\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\n",
      "\u001b[36malgo-1-zwl3k_1  |\u001b[0m 2019-07-16 15:27:04,144 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpsw39_nhj_algo-1-zwl3k_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "inputs = {'train': f'file://{train_dir}',\n",
    "          'test': f'file://{test_dir}'}\n",
    "\n",
    "local_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hosted Training\n",
    "\n",
    "After we've confirmed our code seems to be working using Local Mode training, we can move on to use SageMaker's hosted training, which uses compute resources separate from your notebook instance.  Hosted training spins up one or more instances (cluster) for training, and then tears the cluster down when training is complete. In general, hosted training is preferred for doing actual training, especially for large-scale, distributed training. Before starting hosted training, the data must be uploaded to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-072677473360/sagemaker-us-east-1-072677473360/data/train', 'test': 's3://sagemaker-us-east-1-072677473360/sagemaker-us-east-1-072677473360/data/test'}\n"
     ]
    }
   ],
   "source": [
    "s3_prefix = sagemaker.Session().default_bucket()\n",
    "\n",
    "traindata_s3_prefix = '{}/data/train'.format(s3_prefix)\n",
    "testdata_s3_prefix = '{}/data/test'.format(s3_prefix)\n",
    "\n",
    "train_s3 = sagemaker.Session().upload_data(path='./data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sagemaker.Session().upload_data(path='./data/test/', key_prefix=testdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'test': test_s3}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training data now in S3, we're ready to set up an Estimator object for hosted training. It is similar to the Local Mode Estimator, except the `train_instance_type` has been set to a ML instance type instead of a local type for Local Mode. Additionally, we've set the number of epochs to a number greater than one for actual training, as opposed to just testing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "#hyperparameters = {'epochs': 10, 'batch_size': 128}\n",
    "hyperparameters = {'epochs': 1, 'batch_size': 128, \n",
    "                   'tornasole-save-interval': 1, 'tornasole_outdir' : tornasole_s3 }\n",
    "\n",
    "estimator = TensorFlow(entry_point='sentiment_keras.py',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-keras-sentiment',\n",
    "                       framework_version='1.13.1',\n",
    "                       py_version='py3',\n",
    "                       image_name='072677473360.dkr.ecr.us-east-1.amazonaws.com/tornasole-preprod-tf-1.13.1-cpu:latest',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the change in training instance type and increase in epochs, we simply call `fit` to start the actual hosted training.  At the end of hosted training, you'll see from the logs below the cell that accuracy on the training set has greatly increased, and accuracy on the validation set is around 90%.  The model may be overfitting now (less able to generalize to data it has not yet seen), even though we are employing dropout as a regularization technique.  In a production situation, further investigation would be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-16 00:36:04 Starting - Starting the training job...\n",
      "2019-07-16 00:36:09 Starting - Launching requested ML instances......\n",
      "2019-07-16 00:37:17 Starting - Preparing the instances for training......\n",
      "2019-07-16 00:38:15 Downloading - Downloading input data......\n",
      "2019-07-16 00:39:27 Training - Downloading the training image......\n",
      "2019-07-16 00:40:17 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m2019-07-16 00:40:20,820 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[31m2019-07-16 00:40:21,423 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"tornasole_outdir\": \"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\",\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 1,\n",
      "        \"tornasole-save-interval\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-keras-sentiment-2019-07-16-00-36-04-131\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-072677473360/tf-keras-sentiment-2019-07-16-00-36-04-131/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sentiment_keras\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sentiment_keras.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"batch_size\":128,\"epochs\":1,\"model_dir\":\"/opt/ml/model\",\"tornasole-save-interval\":1,\"tornasole_outdir\":\"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=sentiment_keras.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=sentiment_keras\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-1-072677473360/tf-keras-sentiment-2019-07-16-00-36-04-131/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":1,\"model_dir\":\"/opt/ml/model\",\"tornasole-save-interval\":1,\"tornasole_outdir\":\"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-keras-sentiment-2019-07-16-00-36-04-131\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-072677473360/tf-keras-sentiment-2019-07-16-00-36-04-131/source/sourcedir.tar.gz\",\"module_name\":\"sentiment_keras\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sentiment_keras.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"1\",\"--model_dir\",\"/opt/ml/model\",\"--tornasole-save-interval\",\"1\",\"--tornasole_outdir\",\"s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[31mSM_HP_TORNASOLE_OUTDIR=s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\u001b[0m\n",
      "\u001b[31mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[31mSM_HP_TORNASOLE-SAVE-INTERVAL=1\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/local/bin/python sentiment_keras.py --batch_size 128 --epochs 1 --model_dir /opt/ml/model --tornasole-save-interval 1 --tornasole_outdir s3://sagemaker-us-east-1-072677473360/tornasole-parameters/\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[31mx train (25000, 400) y train (25000,)\u001b[0m\n",
      "\u001b[31m[[  0   0   0 ...  19 178  32]\n",
      " [  0   0   0 ...  16 145  95]\n",
      " [  0   0   0 ...   7 129 113]\n",
      " ...\n",
      " [595  13 258 ...  72  33  32]\n",
      " [  0   0   0 ...  28 126 110]\n",
      " [  0   0   0 ...   7  43  50]]\u001b[0m\n",
      "\u001b[31m[1 0 0 1 0 0 1 0 1 0]\u001b[0m\n",
      "\u001b[31mx test (25000, 400) y test (25000,)\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mColocations handled automatically by placer.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mColocations handled automatically by placer.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse tf.cast instead.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse tf.cast instead.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mDeprecated in favor of operator or tf.math.divide.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mDeprecated in favor of operator or tf.math.divide.\u001b[0m\n",
      "\u001b[31mTrain on 25000 samples, validate on 25000 samples\u001b[0m\n",
      "\u001b[31mEpoch 1/1\u001b[0m\n",
      "\u001b[31m  128/25000 [..............................] - ETA: 5:29 - loss: 0.6979 - acc: 0.4531\u001b[0m\n",
      "\u001b[31m  256/25000 [..............................] - ETA: 5:21 - loss: 0.6944 - acc: 0.5000\n",
      "  384/25000 [..............................] - ETA: 4:22 - loss: 0.7009 - acc: 0.4922\u001b[0m\n",
      "\u001b[31m  512/25000 [..............................] - ETA: 3:52 - loss: 0.7005 - acc: 0.4922\u001b[0m\n",
      "\u001b[31m  640/25000 [..............................] - ETA: 3:34 - loss: 0.6990 - acc: 0.4875\u001b[0m\n",
      "\u001b[31m  768/25000 [..............................] - ETA: 3:22 - loss: 0.6979 - acc: 0.4935\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m  896/25000 [>.............................] - ETA: 3:13 - loss: 0.6988 - acc: 0.4922\n",
      " 1024/25000 [>.............................] - ETA: 3:06 - loss: 0.6980 - acc: 0.4961\u001b[0m\n",
      "\u001b[31m 1152/25000 [>.............................] - ETA: 3:00 - loss: 0.6998 - acc: 0.4931\u001b[0m\n",
      "\u001b[31m 1280/25000 [>.............................] - ETA: 2:56 - loss: 0.7008 - acc: 0.4883\n",
      " 1408/25000 [>.............................] - ETA: 3:06 - loss: 0.7003 - acc: 0.4879\u001b[0m\n",
      "\u001b[31m 1536/25000 [>.............................] - ETA: 3:01 - loss: 0.6993 - acc: 0.4961\u001b[0m\n",
      "\u001b[31m 1664/25000 [>.............................] - ETA: 2:57 - loss: 0.6984 - acc: 0.5072\u001b[0m\n",
      "\u001b[31m 1792/25000 [=>............................] - ETA: 2:53 - loss: 0.6981 - acc: 0.5056\u001b[0m\n",
      "\u001b[31m 1920/25000 [=>............................] - ETA: 2:50 - loss: 0.6969 - acc: 0.5120\n",
      " 2048/25000 [=>............................] - ETA: 2:48 - loss: 0.6969 - acc: 0.5112\u001b[0m\n",
      "\u001b[31m 2176/25000 [=>............................] - ETA: 2:45 - loss: 0.6969 - acc: 0.5078\u001b[0m\n",
      "\u001b[31m 2304/25000 [=>............................] - ETA: 2:43 - loss: 0.6966 - acc: 0.5074\u001b[0m\n",
      "\u001b[31m 2432/25000 [=>............................] - ETA: 2:40 - loss: 0.6965 - acc: 0.5053\u001b[0m\n",
      "\u001b[31m 2560/25000 [==>...........................] - ETA: 2:45 - loss: 0.6960 - acc: 0.5059\u001b[0m\n",
      "\u001b[31m 2688/25000 [==>...........................] - ETA: 2:43 - loss: 0.6956 - acc: 0.5089\u001b[0m\n",
      "\u001b[31m 2816/25000 [==>...........................] - ETA: 2:40 - loss: 0.6950 - acc: 0.5142\n",
      " 2944/25000 [==>...........................] - ETA: 2:38 - loss: 0.6947 - acc: 0.5166\u001b[0m\n",
      "\u001b[31m 3072/25000 [==>...........................] - ETA: 2:36 - loss: 0.6940 - acc: 0.5208\u001b[0m\n",
      "\u001b[31m 3200/25000 [==>...........................] - ETA: 2:34 - loss: 0.6936 - acc: 0.5209\u001b[0m\n",
      "\u001b[31m 3328/25000 [==>...........................] - ETA: 2:33 - loss: 0.6932 - acc: 0.5216\n",
      " 3456/25000 [===>..........................] - ETA: 2:31 - loss: 0.6930 - acc: 0.5214\u001b[0m\n",
      "\u001b[31m 3584/25000 [===>..........................] - ETA: 2:29 - loss: 0.6928 - acc: 0.5209\u001b[0m\n",
      "\u001b[31m 3712/25000 [===>..........................] - ETA: 2:33 - loss: 0.6923 - acc: 0.5229\u001b[0m\n",
      "\u001b[31m 3840/25000 [===>..........................] - ETA: 2:31 - loss: 0.6915 - acc: 0.5289\n",
      " 3968/25000 [===>..........................] - ETA: 2:30 - loss: 0.6906 - acc: 0.5358\u001b[0m\n",
      "\u001b[31m 4096/25000 [===>..........................] - ETA: 2:28 - loss: 0.6901 - acc: 0.5386\u001b[0m\n",
      "\u001b[31m 4224/25000 [====>.........................] - ETA: 2:27 - loss: 0.6895 - acc: 0.5400\u001b[0m\n",
      "\u001b[31m 4352/25000 [====>.........................] - ETA: 2:25 - loss: 0.6892 - acc: 0.5402\u001b[0m\n",
      "\u001b[31m 4480/25000 [====>.........................] - ETA: 2:24 - loss: 0.6883 - acc: 0.5442\n",
      " 4608/25000 [====>.........................] - ETA: 2:22 - loss: 0.6875 - acc: 0.5493\u001b[0m\n",
      "\u001b[31m 4736/25000 [====>.........................] - ETA: 2:21 - loss: 0.6866 - acc: 0.5536\u001b[0m\n",
      "\u001b[31m 4864/25000 [====>.........................] - ETA: 2:22 - loss: 0.6855 - acc: 0.5588\n",
      " 4992/25000 [====>.........................] - ETA: 2:21 - loss: 0.6845 - acc: 0.5623\u001b[0m\n",
      "\u001b[31m 5120/25000 [=====>........................] - ETA: 2:19 - loss: 0.6836 - acc: 0.5652\u001b[0m\n",
      "\u001b[31m 5248/25000 [=====>........................] - ETA: 2:18 - loss: 0.6827 - acc: 0.5665\u001b[0m\n",
      "\u001b[31m 5376/25000 [=====>........................] - ETA: 2:17 - loss: 0.6825 - acc: 0.5664\n",
      " 5504/25000 [=====>........................] - ETA: 2:15 - loss: 0.6816 - acc: 0.5672\u001b[0m\n",
      "\u001b[31m 5632/25000 [=====>........................] - ETA: 2:14 - loss: 0.6806 - acc: 0.5701\u001b[0m\n",
      "\u001b[31m 5760/25000 [=====>........................] - ETA: 2:13 - loss: 0.6795 - acc: 0.5724\u001b[0m\n",
      "\u001b[31m 5888/25000 [======>.......................] - ETA: 2:11 - loss: 0.6785 - acc: 0.5727\u001b[0m\n",
      "\u001b[31m 6016/25000 [======>.......................] - ETA: 2:12 - loss: 0.6762 - acc: 0.5751\u001b[0m\n",
      "\u001b[31m 6144/25000 [======>.......................] - ETA: 2:11 - loss: 0.6753 - acc: 0.5778\u001b[0m\n",
      "\u001b[31m 6272/25000 [======>.......................] - ETA: 2:10 - loss: 0.6736 - acc: 0.5818\u001b[0m\n",
      "\u001b[31m 6400/25000 [======>.......................] - ETA: 2:09 - loss: 0.6719 - acc: 0.5850\n",
      " 6528/25000 [======>.......................] - ETA: 2:07 - loss: 0.6705 - acc: 0.5879\u001b[0m\n",
      "\u001b[31m 6656/25000 [======>.......................] - ETA: 2:06 - loss: 0.6688 - acc: 0.5898\u001b[0m\n",
      "\u001b[31m 6784/25000 [=======>......................] - ETA: 2:05 - loss: 0.6678 - acc: 0.5909\u001b[0m\n",
      "\u001b[31m 6912/25000 [=======>......................] - ETA: 2:04 - loss: 0.6661 - acc: 0.5926\u001b[0m\n",
      "\u001b[31m 7040/25000 [=======>......................] - ETA: 2:03 - loss: 0.6641 - acc: 0.5938\u001b[0m\n",
      "\u001b[31m 7168/25000 [=======>......................] - ETA: 2:03 - loss: 0.6620 - acc: 0.5958\u001b[0m\n",
      "\u001b[31m 7296/25000 [=======>......................] - ETA: 2:02 - loss: 0.6612 - acc: 0.5973\n",
      " 7424/25000 [=======>......................] - ETA: 2:01 - loss: 0.6589 - acc: 0.6001\u001b[0m\n",
      "\u001b[31m 7552/25000 [========>.....................] - ETA: 2:00 - loss: 0.6573 - acc: 0.6029\u001b[0m\n",
      "\u001b[31m 7680/25000 [========>.....................] - ETA: 1:59 - loss: 0.6552 - acc: 0.6065\u001b[0m\n",
      "\u001b[31m 7808/25000 [========>.....................] - ETA: 1:58 - loss: 0.6532 - acc: 0.6092\n",
      " 7936/25000 [========>.....................] - ETA: 1:57 - loss: 0.6502 - acc: 0.6129\u001b[0m\n",
      "\u001b[31m 8064/25000 [========>.....................] - ETA: 1:56 - loss: 0.6481 - acc: 0.6152\u001b[0m\n",
      "\u001b[31m 8192/25000 [========>.....................] - ETA: 1:55 - loss: 0.6467 - acc: 0.6161\u001b[0m\n",
      "\u001b[31m 8320/25000 [========>.....................] - ETA: 1:55 - loss: 0.6445 - acc: 0.6178\n",
      " 8448/25000 [=========>....................] - ETA: 1:54 - loss: 0.6423 - acc: 0.6197\u001b[0m\n",
      "\u001b[31m 8576/25000 [=========>....................] - ETA: 1:53 - loss: 0.6404 - acc: 0.6212\u001b[0m\n",
      "\u001b[31m 8704/25000 [=========>....................] - ETA: 1:52 - loss: 0.6386 - acc: 0.6229\u001b[0m\n",
      "\u001b[31m 8832/25000 [=========>....................] - ETA: 1:51 - loss: 0.6363 - acc: 0.6248\n",
      " 8960/25000 [=========>....................] - ETA: 1:50 - loss: 0.6334 - acc: 0.6282\u001b[0m\n",
      "\u001b[31m 9088/25000 [=========>....................] - ETA: 1:49 - loss: 0.6303 - acc: 0.6316\u001b[0m\n",
      "\u001b[31m 9216/25000 [==========>...................] - ETA: 1:47 - loss: 0.6284 - acc: 0.6331\u001b[0m\n",
      "\u001b[31m 9344/25000 [==========>...................] - ETA: 1:46 - loss: 0.6252 - acc: 0.6360\n",
      " 9472/25000 [==========>...................] - ETA: 1:47 - loss: 0.6222 - acc: 0.6388\u001b[0m\n",
      "\u001b[31m 9600/25000 [==========>...................] - ETA: 1:46 - loss: 0.6206 - acc: 0.6408\u001b[0m\n",
      "\u001b[31m 9728/25000 [==========>...................] - ETA: 1:45 - loss: 0.6181 - acc: 0.6430\u001b[0m\n",
      "\u001b[31m 9856/25000 [==========>...................] - ETA: 1:44 - loss: 0.6151 - acc: 0.6452\n",
      " 9984/25000 [==========>...................] - ETA: 1:42 - loss: 0.6125 - acc: 0.6473\u001b[0m\n",
      "\u001b[31m10112/25000 [===========>..................] - ETA: 1:41 - loss: 0.6102 - acc: 0.6494\u001b[0m\n",
      "\u001b[31m10240/25000 [===========>..................] - ETA: 1:40 - loss: 0.6082 - acc: 0.6511\u001b[0m\n",
      "\u001b[31m10368/25000 [===========>..................] - ETA: 1:39 - loss: 0.6056 - acc: 0.6529\u001b[0m\n",
      "\u001b[31m10496/25000 [===========>..................] - ETA: 1:38 - loss: 0.6038 - acc: 0.6548\u001b[0m\n",
      "\u001b[31m10624/25000 [===========>..................] - ETA: 1:47 - loss: 0.6021 - acc: 0.6558\u001b[0m\n",
      "\u001b[31m10752/25000 [===========>..................] - ETA: 1:46 - loss: 0.6010 - acc: 0.6566\u001b[0m\n",
      "\u001b[31m10880/25000 [============>.................] - ETA: 1:44 - loss: 0.5986 - acc: 0.6590\u001b[0m\n",
      "\u001b[31m11008/25000 [============>.................] - ETA: 1:43 - loss: 0.5961 - acc: 0.6611\u001b[0m\n",
      "\u001b[31m11136/25000 [============>.................] - ETA: 1:42 - loss: 0.5938 - acc: 0.6629\u001b[0m\n",
      "\u001b[31m11264/25000 [============>.................] - ETA: 1:41 - loss: 0.5906 - acc: 0.6652\u001b[0m\n",
      "\u001b[31m11392/25000 [============>.................] - ETA: 1:40 - loss: 0.5890 - acc: 0.6664\u001b[0m\n",
      "\u001b[31m11520/25000 [============>.................] - ETA: 1:39 - loss: 0.5871 - acc: 0.6682\u001b[0m\n",
      "\u001b[31m11648/25000 [============>.................] - ETA: 1:38 - loss: 0.5853 - acc: 0.6696\u001b[0m\n",
      "\u001b[31m11776/25000 [=============>................] - ETA: 1:37 - loss: 0.5827 - acc: 0.6713\u001b[0m\n",
      "\u001b[31m11904/25000 [=============>................] - ETA: 1:36 - loss: 0.5808 - acc: 0.6728\u001b[0m\n",
      "\u001b[31m12032/25000 [=============>................] - ETA: 1:35 - loss: 0.5778 - acc: 0.6750\u001b[0m\n",
      "\u001b[31m12160/25000 [=============>................] - ETA: 1:34 - loss: 0.5753 - acc: 0.6766\u001b[0m\n",
      "\u001b[31m12288/25000 [=============>................] - ETA: 1:33 - loss: 0.5731 - acc: 0.6785\u001b[0m\n",
      "\u001b[31m12416/25000 [=============>................] - ETA: 1:32 - loss: 0.5699 - acc: 0.6807\u001b[0m\n",
      "\u001b[31m12544/25000 [==============>...............] - ETA: 1:31 - loss: 0.5679 - acc: 0.6818\u001b[0m\n",
      "\u001b[31m12672/25000 [==============>...............] - ETA: 1:29 - loss: 0.5656 - acc: 0.6836\u001b[0m\n",
      "\u001b[31m12800/25000 [==============>...............] - ETA: 1:28 - loss: 0.5631 - acc: 0.6855\u001b[0m\n",
      "\u001b[31m12928/25000 [==============>...............] - ETA: 1:28 - loss: 0.5616 - acc: 0.6863\u001b[0m\n",
      "\u001b[31m13056/25000 [==============>...............] - ETA: 1:27 - loss: 0.5592 - acc: 0.6882\u001b[0m\n",
      "\u001b[31m13184/25000 [==============>...............] - ETA: 1:26 - loss: 0.5569 - acc: 0.6899\u001b[0m\n",
      "\u001b[31m13312/25000 [==============>...............] - ETA: 1:25 - loss: 0.5541 - acc: 0.6919\u001b[0m\n",
      "\u001b[31m13440/25000 [===============>..............] - ETA: 1:24 - loss: 0.5518 - acc: 0.6936\u001b[0m\n",
      "\u001b[31m13568/25000 [===============>..............] - ETA: 1:23 - loss: 0.5508 - acc: 0.6944\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m13696/25000 [===============>..............] - ETA: 1:22 - loss: 0.5482 - acc: 0.6962\u001b[0m\n",
      "\u001b[31m13824/25000 [===============>..............] - ETA: 1:21 - loss: 0.5464 - acc: 0.6970\u001b[0m\n",
      "\u001b[31m13952/25000 [===============>..............] - ETA: 1:20 - loss: 0.5452 - acc: 0.6978\u001b[0m\n",
      "\u001b[31m14080/25000 [===============>..............] - ETA: 1:20 - loss: 0.5433 - acc: 0.6993\u001b[0m\n",
      "\u001b[31m14208/25000 [================>.............] - ETA: 1:18 - loss: 0.5414 - acc: 0.7010\u001b[0m\n",
      "\u001b[31m14336/25000 [================>.............] - ETA: 1:17 - loss: 0.5393 - acc: 0.7026\u001b[0m\n",
      "\u001b[31m14464/25000 [================>.............] - ETA: 1:16 - loss: 0.5380 - acc: 0.7038\u001b[0m\n",
      "\u001b[31m14592/25000 [================>.............] - ETA: 1:15 - loss: 0.5368 - acc: 0.7047\u001b[0m\n",
      "\u001b[31m14720/25000 [================>.............] - ETA: 1:14 - loss: 0.5347 - acc: 0.7064\u001b[0m\n",
      "\u001b[31m14848/25000 [================>.............] - ETA: 1:13 - loss: 0.5331 - acc: 0.7076\u001b[0m\n",
      "\u001b[31m14976/25000 [================>.............] - ETA: 1:12 - loss: 0.5311 - acc: 0.7091\u001b[0m\n",
      "\u001b[31m15104/25000 [=================>............] - ETA: 1:11 - loss: 0.5295 - acc: 0.7103\u001b[0m\n",
      "\u001b[31m15232/25000 [=================>............] - ETA: 1:11 - loss: 0.5276 - acc: 0.7119\u001b[0m\n",
      "\u001b[31m15360/25000 [=================>............] - ETA: 1:10 - loss: 0.5262 - acc: 0.7130\u001b[0m\n",
      "\u001b[31m15488/25000 [=================>............] - ETA: 1:09 - loss: 0.5244 - acc: 0.7143\u001b[0m\n",
      "\u001b[31m15616/25000 [=================>............] - ETA: 1:08 - loss: 0.5229 - acc: 0.7154\u001b[0m\n",
      "\u001b[31m15744/25000 [=================>............] - ETA: 1:07 - loss: 0.5214 - acc: 0.7167\u001b[0m\n",
      "\u001b[31m15872/25000 [==================>...........] - ETA: 1:06 - loss: 0.5203 - acc: 0.7176\u001b[0m\n",
      "\u001b[31m16000/25000 [==================>...........] - ETA: 1:05 - loss: 0.5199 - acc: 0.7180\u001b[0m\n",
      "\u001b[31m16128/25000 [==================>...........] - ETA: 1:04 - loss: 0.5182 - acc: 0.7193\u001b[0m\n",
      "\u001b[31m16256/25000 [==================>...........] - ETA: 1:03 - loss: 0.5165 - acc: 0.7204\u001b[0m\n",
      "\u001b[31m16384/25000 [==================>...........] - ETA: 1:02 - loss: 0.5148 - acc: 0.7215\u001b[0m\n",
      "\u001b[31m16512/25000 [==================>...........] - ETA: 1:01 - loss: 0.5137 - acc: 0.7224\u001b[0m\n",
      "\u001b[31m16640/25000 [==================>...........] - ETA: 1:00 - loss: 0.5131 - acc: 0.7231\u001b[0m\n",
      "\u001b[31m16768/25000 [===================>..........] - ETA: 59s - loss: 0.5115 - acc: 0.7245 \u001b[0m\n",
      "\u001b[31m16896/25000 [===================>..........] - ETA: 58s - loss: 0.5100 - acc: 0.7253\u001b[0m\n",
      "\u001b[31m17024/25000 [===================>..........] - ETA: 57s - loss: 0.5084 - acc: 0.7263\u001b[0m\n",
      "\u001b[31m17152/25000 [===================>..........] - ETA: 56s - loss: 0.5066 - acc: 0.7275\u001b[0m\n",
      "\u001b[31m17280/25000 [===================>..........] - ETA: 55s - loss: 0.5051 - acc: 0.7284\u001b[0m\n",
      "\u001b[31m17408/25000 [===================>..........] - ETA: 54s - loss: 0.5039 - acc: 0.7293\u001b[0m\n",
      "\u001b[31m17536/25000 [====================>.........] - ETA: 54s - loss: 0.5020 - acc: 0.7304\u001b[0m\n",
      "\u001b[31m17664/25000 [====================>.........] - ETA: 53s - loss: 0.5007 - acc: 0.7312\u001b[0m\n",
      "\u001b[31m17792/25000 [====================>.........] - ETA: 52s - loss: 0.4996 - acc: 0.7321\u001b[0m\n",
      "\u001b[31m17920/25000 [====================>.........] - ETA: 51s - loss: 0.4980 - acc: 0.7333\u001b[0m\n",
      "\u001b[31m18048/25000 [====================>.........] - ETA: 50s - loss: 0.4969 - acc: 0.7340\u001b[0m\n",
      "\u001b[31m18176/25000 [====================>.........] - ETA: 49s - loss: 0.4952 - acc: 0.7351\u001b[0m\n",
      "\u001b[31m18304/25000 [====================>.........] - ETA: 48s - loss: 0.4939 - acc: 0.7360\u001b[0m\n",
      "\u001b[31m18432/25000 [=====================>........] - ETA: 47s - loss: 0.4935 - acc: 0.7368\u001b[0m\n",
      "\u001b[31m18560/25000 [=====================>........] - ETA: 46s - loss: 0.4916 - acc: 0.7379\u001b[0m\n",
      "\u001b[31m18688/25000 [=====================>........] - ETA: 45s - loss: 0.4902 - acc: 0.7388\u001b[0m\n",
      "\u001b[31m18816/25000 [=====================>........] - ETA: 44s - loss: 0.4889 - acc: 0.7396\u001b[0m\n",
      "\u001b[31m18944/25000 [=====================>........] - ETA: 43s - loss: 0.4881 - acc: 0.7401\u001b[0m\n",
      "\u001b[31m19072/25000 [=====================>........] - ETA: 42s - loss: 0.4864 - acc: 0.7412\u001b[0m\n",
      "\u001b[31m19200/25000 [======================>.......] - ETA: 41s - loss: 0.4855 - acc: 0.7419\u001b[0m\n",
      "\u001b[31m19328/25000 [======================>.......] - ETA: 40s - loss: 0.4839 - acc: 0.7429\u001b[0m\n",
      "\u001b[31m19456/25000 [======================>.......] - ETA: 39s - loss: 0.4825 - acc: 0.7438\u001b[0m\n",
      "\u001b[31m19584/25000 [======================>.......] - ETA: 38s - loss: 0.4815 - acc: 0.7446\u001b[0m\n",
      "\u001b[31m19712/25000 [======================>.......] - ETA: 37s - loss: 0.4803 - acc: 0.7455\u001b[0m\n",
      "\u001b[31m19840/25000 [======================>.......] - ETA: 36s - loss: 0.4792 - acc: 0.7463\u001b[0m\n",
      "\u001b[31m19968/25000 [======================>.......] - ETA: 36s - loss: 0.4777 - acc: 0.7475\u001b[0m\n",
      "\u001b[31m20096/25000 [=======================>......] - ETA: 35s - loss: 0.4773 - acc: 0.7479\u001b[0m\n",
      "\u001b[31m20224/25000 [=======================>......] - ETA: 34s - loss: 0.4757 - acc: 0.7490\u001b[0m\n",
      "\u001b[31m20352/25000 [=======================>......] - ETA: 33s - loss: 0.4742 - acc: 0.7500\u001b[0m\n",
      "\u001b[31m20480/25000 [=======================>......] - ETA: 32s - loss: 0.4730 - acc: 0.7506\u001b[0m\n",
      "\u001b[31m20608/25000 [=======================>......] - ETA: 31s - loss: 0.4729 - acc: 0.7511\u001b[0m\n",
      "\u001b[31m20736/25000 [=======================>......] - ETA: 30s - loss: 0.4719 - acc: 0.7517\u001b[0m\n",
      "\u001b[31m20864/25000 [========================>.....] - ETA: 29s - loss: 0.4710 - acc: 0.7523\u001b[0m\n",
      "\u001b[31m20992/25000 [========================>.....] - ETA: 29s - loss: 0.4697 - acc: 0.7534\u001b[0m\n",
      "\u001b[31m21120/25000 [========================>.....] - ETA: 28s - loss: 0.4686 - acc: 0.7541\u001b[0m\n",
      "\u001b[31m21248/25000 [========================>.....] - ETA: 27s - loss: 0.4673 - acc: 0.7548\u001b[0m\n",
      "\u001b[31m21376/25000 [========================>.....] - ETA: 26s - loss: 0.4664 - acc: 0.7554\u001b[0m\n",
      "\u001b[31m21504/25000 [========================>.....] - ETA: 25s - loss: 0.4654 - acc: 0.7560\u001b[0m\n",
      "\u001b[31m21632/25000 [========================>.....] - ETA: 24s - loss: 0.4644 - acc: 0.7571\u001b[0m\n",
      "\u001b[31m21760/25000 [=========================>....] - ETA: 23s - loss: 0.4632 - acc: 0.7579\u001b[0m\n",
      "\u001b[31m21888/25000 [=========================>....] - ETA: 22s - loss: 0.4619 - acc: 0.7589\u001b[0m\n",
      "\u001b[31m22016/25000 [=========================>....] - ETA: 21s - loss: 0.4608 - acc: 0.7597\u001b[0m\n",
      "\u001b[31m22144/25000 [=========================>....] - ETA: 21s - loss: 0.4598 - acc: 0.7603\u001b[0m\n",
      "\u001b[31m22272/25000 [=========================>....] - ETA: 20s - loss: 0.4589 - acc: 0.7608\u001b[0m\n",
      "\u001b[31m22400/25000 [=========================>....] - ETA: 19s - loss: 0.4581 - acc: 0.7615\u001b[0m\n",
      "\u001b[31m22528/25000 [==========================>...] - ETA: 18s - loss: 0.4567 - acc: 0.7625\u001b[0m\n",
      "\u001b[31m22656/25000 [==========================>...] - ETA: 17s - loss: 0.4562 - acc: 0.7629\u001b[0m\n",
      "\u001b[31m22784/25000 [==========================>...] - ETA: 16s - loss: 0.4553 - acc: 0.7636\u001b[0m\n",
      "\u001b[31m22912/25000 [==========================>...] - ETA: 15s - loss: 0.4540 - acc: 0.7645\u001b[0m\n",
      "\u001b[31m23040/25000 [==========================>...] - ETA: 14s - loss: 0.4530 - acc: 0.7653\u001b[0m\n",
      "\u001b[31m23168/25000 [==========================>...] - ETA: 13s - loss: 0.4524 - acc: 0.7659\u001b[0m\n",
      "\u001b[31m23296/25000 [==========================>...] - ETA: 12s - loss: 0.4512 - acc: 0.7667\u001b[0m\n",
      "\u001b[31m23424/25000 [===========================>..] - ETA: 11s - loss: 0.4500 - acc: 0.7672\u001b[0m\n",
      "\u001b[31m23552/25000 [===========================>..] - ETA: 10s - loss: 0.4489 - acc: 0.7678\u001b[0m\n",
      "\u001b[31m23680/25000 [===========================>..] - ETA: 9s - loss: 0.4477 - acc: 0.7686 \u001b[0m\n",
      "\u001b[31m23808/25000 [===========================>..] - ETA: 8s - loss: 0.4464 - acc: 0.7695\u001b[0m\n",
      "\u001b[31m23936/25000 [===========================>..] - ETA: 7s - loss: 0.4454 - acc: 0.7703\u001b[0m\n",
      "\u001b[31m24064/25000 [===========================>..] - ETA: 6s - loss: 0.4448 - acc: 0.7707\u001b[0m\n",
      "\u001b[31m24192/25000 [============================>.] - ETA: 5s - loss: 0.4441 - acc: 0.7710\u001b[0m\n",
      "\u001b[31m24320/25000 [============================>.] - ETA: 5s - loss: 0.4437 - acc: 0.7714\u001b[0m\n",
      "\u001b[31m24448/25000 [============================>.] - ETA: 4s - loss: 0.4430 - acc: 0.7719\u001b[0m\n",
      "\u001b[31m24576/25000 [============================>.] - ETA: 3s - loss: 0.4420 - acc: 0.7726\u001b[0m\n",
      "\u001b[31m24704/25000 [============================>.] - ETA: 2s - loss: 0.4411 - acc: 0.7733\u001b[0m\n",
      "\u001b[31m24832/25000 [============================>.] - ETA: 1s - loss: 0.4403 - acc: 0.7739\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m24960/25000 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.7745\u001b[0m\n",
      "\u001b[31m25000/25000 [==============================] - 233s 9ms/step - loss: 0.4387 - acc: 0.7748 - val_loss: 0.2636 - val_acc: 0.8898\u001b[0m\n",
      "\u001b[31m2019-07-16 00:44:17,528 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container.The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[31mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[31m2019-07-16 00:44:17,528 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-07-16 00:45:32 Uploading - Uploading generated training model\n",
      "2019-07-16 00:45:32 Completed - Training job completed\n",
      "Billable seconds: 437\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Prediction\n",
    "\n",
    "\n",
    "If our use case requires individual predictions in near real-time, SageMaker hosted endpoints can be created. Hosted endpoints also can be used for pseudo-batch prediction, but the process is more involved than simply using SageMaker's Batch Transform feature, which is designed for large-scale, asynchronous batch inference.\n",
    "\n",
    "To use Batch Transform, first we must upload to Amazon S3 some test data in CSV format to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvtestdata_s3_prefix = '{}/data/csv-test'.format(s3_prefix)\n",
    "csvtest_s3 = sagemaker.Session().upload_data(path='./data/csv-test/', key_prefix=csvtestdata_s3_prefix)\n",
    "print(csvtest_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Transformer object must be set up to describe the Batch Transform job, including the amount and type of inference hardware to be used.  Then the actual transform job itself is started with a call to the `transform` method of the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(instance_count=1, instance_type='ml.m5.xlarge')\n",
    "transformer.transform(csvtest_s3, content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now download the batch predictions from S3 to the local filesystem on the notebook instance; the predictions are contained in a file with a .out extension, and are embedded in JSON.  Next we'll load the JSON and examine the predictions, which are confidence scores from 0.0 to 1.0 where numbers close to 1.0 indicate positive sentiment, while numbers close to 0.0 indicate negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "batch_output = transformer.output_path\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/\n",
    "\n",
    "with open('batch_data/output/csv-test.csv.out', 'r') as f:\n",
    "    jstr = json.load(f)\n",
    "    results = [float('%.3f'%(item)) for sublist in jstr['predictions'] for item in sublist]\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the text of some actual reviews to see the predictions in action.  First, we have to convert the integers representing the words back to the words themselves by using a reversed dictionary.  Next we can decode the reviews, taking into account that the first 3 indices were reserved for \"padding\", \"start of sequence\", and \"unknown\", and removing a string of unknown tokens from the start of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r'^[\\?\\s]+')\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "first_decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_test[3]])\n",
    "regex.sub('', first_decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this review looks fairly negative.  Let's compare the actual label with the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    return 'positive' if score > 0.5 else 'negative' \n",
    "\n",
    "print('Labeled sentiment for this review is {}, predicted sentiment is {}'.format(get_sentiment(y_test[3]), \n",
    "                                                                                  get_sentiment(results[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our negative sentiment prediction agrees with the label for this review.  Let's now examine another review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_test[10]])\n",
    "regex.sub('', second_decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labeled sentiment for this review is {}, predicted sentiment is {}'.format(get_sentiment(y_test[10]), \n",
    "                                                                                  get_sentiment(results[10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the prediction agreed with the label for the test data.  Note that there is no need to clean up any Batch Transform resources:  after the transform job is complete, the cluster used to make inferences is torn down.\n",
    "\n",
    "Now that we've reviewed some sample predictions as a sanity check, we're finished.  Of course, in a typical production situation, the data science project lifecycle is iterative, with repeated cycles of refining the model using a tool such as Amazon SageMaker's Automatic Model Tuning feature, and gathering more data.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
