{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging SageMaker XGBoost Training Jobs with Tornasole\n",
    "\n",
    "This notebook uses the MNIST dataset to demonstrate a classification task using Tornasole with XGBoost.\n",
    "For a regression problem, see [xgboost_regression.ipynb](xgboost_regression.ipynb).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Tornasole is a new capability of Amazon SageMaker that allows debugging machine learning training. \n",
    "Tornasole helps you to monitor your training in near real time using rules and would provide you\n",
    "alerts, once it has detected inconsistency in training. \n",
    "\n",
    "Using Tornasole is a two step process: Saving tensors and Analysis.\n",
    "Let's look at each one of them closely.\n",
    "\n",
    "### Saving tensors (and scalars)\n",
    "\n",
    "In deep learning algorithms, tensors define the state of the training job\n",
    "at any particular instant in its lifecycle.\n",
    "Tornasole exposes a library which allows you to capture these tensors and\n",
    "save them for analysis.\n",
    "Although XGBoost is not a deep learning algorithm, Tornasole is highly customizable\n",
    "and can help provide interpretability by saving insightful metrics, such as\n",
    "performance metrics or feature importances, at different frequencies.\n",
    "Refer to [DeveloperGuide_XGBoost](../DeveloperGuide_XG.md) for details on how to\n",
    "save the metrics you want.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Analysis of the tensors emitted is captured by the Tornasole concept called ***Rules***.\n",
    "On a very broad level, a rule is a python code used to detect certain conditions during training.\n",
    "Some of the conditions that a data scientist training an algorithm may care about are\n",
    "monitoring for gradients getting too large or too small, detecting overfitting, and so on.\n",
    "Tornasole will come pre-packaged with certain rules.\n",
    "Users can write their own rules using the Tornasole APIs.\n",
    "You can also analyze raw tensor data outside of the Rules construct in say, a Sagemaker notebook,\n",
    "using Tornasole's full set of APIs. \n",
    "Please refer to [DeveloperGuide_Rules](../../../rules/DeveloperGuide_Rules.md) for more details about analysis.\n",
    "\n",
    "This example guides you through installation of the required components for emitting tensors in a \n",
    "SageMaker training job and applying a rule over the tensors to monitor the live status of the job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will also install the required tools which will allow emission of tensors (saving tensors) and application of rules to analyze them. This is only for the purposes of this private beta. Once we do this, we will be ready to use Tornasole.\n",
    "\n",
    "You'll probably have to restart this notebook after running the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 sync s3://tornasole-external-preview-use1/sdk/ ~/SageMaker/tornasole-preview-sdk/\n",
    "! pip3 -q install ~/SageMaker/tornasole-preview-sdk/ts-binaries/tornasole_xgboost/py3/latest/tornasole-* --user\n",
    "! chmod +x ~/SageMaker/tornasole-preview-sdk/installer.sh && ~/SageMaker/tornasole-preview-sdk/installer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you running this notebook for the first time, please wait for the above setup to complete and restart the notebook by selecting *Kernel -> Restart Kernel* before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built SageMaker XGBoost containers with Tornasole. You can use them from ECR from SageMaker. Here are the links to the images. Please use the image from the appropriate region in which you want your jobs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Below changes the region to be one where this notebook is running\n",
    "REGION = boto3.Session().region_name\n",
    "ROLE = get_execution_role()\n",
    "os.environ[\"AWS_REGION\"] = REGION\n",
    "\n",
    "TAG = \"latest\"\n",
    "docker_image_name = \"072677473360.dkr.ecr.{}.amazonaws.com/tornasole-preprod-xgboost-0.90-cpu:{}\".format(REGION, TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XGBoost models in SageMaker with Tornasole\n",
    "\n",
    "### SageMaker XGBoost as a framwork\n",
    "\n",
    "We'll train a few XGBoost models in this notebook with Tornasole enabled and monitor the training jobs with Tornasole Rules. This will be done using SageMaker XGBoost 0.90 Container as a framework. The [XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) can be used as a built-in algorithm or as a framework such TensorFlow. Using XGBoost as a framework provides more flexibility than using it as a built-in algorithm as it enables more advanced scenarios that allow pre-processing and post-processing scripts to be incorporated into your training script.\n",
    "\n",
    "Let us first train a simple example training script [xgboost_mnist_basic_hook_demo.py](../scripts/xgboost_abalone_basic_hook_demo.py) with XGBoost enabled in SageMaker using the SageMaker Estimator API, along with a LossNotDecreasing Rule to monitor the training job in realtime. A Tornasole Rule is essentially python code which analyzes tensors saved by tornasole and validates some condition. LossNotDecreasing rule is a first party (1P) rule provided by Tornasole. For other 1P rules that can be used in XGBoost, refer to [FirstPartyRules.md](../../../rules/FirstPartyRules.md)\n",
    "\n",
    "During training, Tornasole will capture tensors as specified in its configuration and LossNotDecreasing Rule job will monitor whether you are running into a situation where loss is not going down. The rule will emit a cloudwatch event if it finds that the performance metrics are not decreasing during training.\n",
    "\n",
    "### Enabling Tornasole in the script\n",
    "\n",
    "You can see in the script that we have made a couple of simple changes to enable Tornasole. We created a TornasoleHook which we pass as a callback function when creating a Booster. We passed a SaveConfig object telling the hook to save the evaluation metrics, feature importances, and SHAP values at regular intervals. Note that Tornasole is highly configurable, you can choose exactly what to save. The changes are described in a bit more detail below after we train this example as well as in even more detail in our [Developer Guide for XGBoost](../DeveloperGuide_XG.md). \n",
    "\n",
    "```python\n",
    "from tornasole.xgboost import TornasoleHook, SaveConfig\n",
    "\n",
    "save_config = SaveConfig(save_interval=frequency)\n",
    "hook = TornasoleHook(save_config=save_config)\n",
    "\n",
    "bst = xgboost.train(\n",
    "    ...\n",
    "    callbacks=[hook]\n",
    ")\n",
    "```\n",
    "\n",
    "### XGBoost for Classification\n",
    "\n",
    "We use the [MNIST data](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html) stored in [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) format.\n",
    "\n",
    "Refer to [XGBoost for Classification](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/xgboost_mnist)\n",
    "for an example of using classification from Amazon SageMaker's implementation of\n",
    "[XGBoost](https://github.com/dmlc/xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point_script = \"../scripts/xgboost_mnist_basic_hook_demo.py\"\n",
    "\n",
    "hyperparameters={\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.5\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"silent\": \"0\",\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": \"10\",  # num_class is required for 'multi:*' objectives\n",
    "    \"num_round\": \"10\",\n",
    "    \"tornasole_frequency\": \"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "estimator = XGBoost(\n",
    "    image_name=docker_image_name,\n",
    "    base_job_name=\"demo-tornasole-xgboost-classification\",\n",
    "    entry_point=entry_point_script,\n",
    "    hyperparameters=hyperparameters,\n",
    "    train_instance_type=\"ml.m4.4xlarge\",\n",
    "    train_instance_count=1,\n",
    "    framework_version=\"0.90-1\",\n",
    "    py_version=\"py3\",\n",
    "    role=ROLE,\n",
    "    \n",
    "    # These are Tornasole specific parameters, \n",
    "    # debug=True means rule specified in rules_specification \n",
    "    # will run as rule job. \n",
    "    # Below, we specify to run the first party rule LossNotDecreasing\n",
    "    # on a ml.c5.4xlarge instance\n",
    "    debug=True,\n",
    "    rules_specification=[\n",
    "        {\n",
    "        \"RuleName\": \"LossNotDecreasing\",\n",
    "        \"InstanceType\": \"ml.c5.4xlarge\",\n",
    "        \"RuntimeConfigurations\": {\n",
    "            \"use_losses_collection\": \"False\",\n",
    "            \"tensor_regex\": \"train-merror,validation-merror\",\n",
    "            \"num_steps\" : \"10\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Note that Tornasole is only supported for `py_version='py3'` currently.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a fire and forget event.\n",
    "# By setting wait=False, we just submit the job to run in the background.\n",
    "# In the background SageMaker will spin off 1 training job and 1 rule job for you.\n",
    "# Please follow this notebook to see status of the training job and the rule job.\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "As a result of the above command, SageMaker will spin off 1 training job and 1 rule job for you - the first one being the job which produces the tensors to be analyzed and the second one, which analyzes the tensors to check if `train-merror` and `validation-merror` are not decreasing at any point during training.\n",
    "\n",
    "### Describing the training job\n",
    "We can check the status of the training job by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below command will give the status of training job\n",
    "# Note: In the output of below command you will see DebugConfig parameter \n",
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The status of the training job can be seen below\n",
    "description[\"TrainingJobStatus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your training job is started SageMaker will spin up a rule execution job to run the LossNotDecreasing rule.\n",
    "\n",
    "### Tornasole specific parameters in the description\n",
    "**DebugConfig** parameter has details about Tornasole related configuration. The key parameters to look for below are\n",
    "\n",
    "*S3OutputPath* : This is the path where output tensors from tornasole is getting saved.  \n",
    "*RuleConfig*' : This parameter tells about the rule config parameter that was passed when creating the trainning job. In this you should be able to see details of the rule that ran for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description[\"DebugConfig\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the status of the Rule Execution Job\n",
    "To get the rule execution job that SageMaker started for you, run the command below and it shows you the `RuleName`, `RuleStatus`, `FailureReason` if any, and `RuleExecutionJobArn`. If the tensors meets a rule evaluation condition, the rule execution job throws a client error with `FailureReason: RuleEvaluationConditionMet`. These details are also available as part of the response `description` above under: `description['RuleMonitoringStatuses']`\n",
    "\n",
    "\n",
    "The logs of the training job are available in the Cloudwatch Logstream `/aws/sagemaker/TrainingJobs` with `RuleExecutionJobArn`. \n",
    "\n",
    "You will see that once the rule execution job starts, that it identifies the loss not decreasing situation in the training job, raises the `RuleEvaluationConditionMet` exception and ends the job. \n",
    "\n",
    "**Note that the next cell blocks until the rule execution job ends. You can stop it at any point to proceed to the rest of the notebook. Once it says RuleStatus is Started, and shows the `RuleExecutionJobArn`, you can look at the status of the rule being monitored. At that point, we can also look at the logs as shown in the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.describe_rule_execution_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check logs of the rule execution jobs\n",
    "\n",
    "If you want to access the logs of a particular rule job name, you can do the following. First, you need to get the rule job name (`RuleExecutionJobArn` field from the training job description). Note that this is only available after the rule job reaches Started stage. Hence the next cell waits till the job name is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "rule_descr = client.describe_training_job(TrainingJobName=job_name)[\"RuleMonitoringStatuses\"]\n",
    "print(\"Waiting for rule execution job to start\")\n",
    "while \"RuleExecutionJobArn\" not in rule_descr[0]:\n",
    "    time.sleep(5)\n",
    "    rule_descr = client.describe_training_job(TrainingJobName=job_name)[\"RuleMonitoringStatuses\"]\n",
    "\n",
    "rule_job_arn = rule_descr[0][\"RuleExecutionJobArn\"]\n",
    "print(\"Rule execution job has started. The job ARN is {}\".format(rule_job_arn))\n",
    "rule_job_name = rule_job_arn.split('/')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can attach to this job to see its logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "loss_not_decreasing = Estimator.attach(rule_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the `LossNotDecreasing` rule was completed without producing an alert because both `train-merror` and `validation-merror` decreased steadily throught the training run. To see an example of the rule when performance metrics stop decreasing during training, see [xgboost_regression.ipynb](xgboost_regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis - Manual\n",
    "\n",
    "Now that we have trained the system we can analyze the data. Here we focus on after-the-fact analysis.\n",
    "\n",
    "We import a basic analysis library, which defines a concept of `Trial` that represents a single training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from tornasole.trials import create_trial\n",
    "\n",
    "s3_output_path = description[\"DebugConfig\"][\"DebugHookConfig\"][\"S3OutputPath\"]\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list all the tensors we know something about. Each one of these names is the name of a tensor - the name is a combination of the feature name (which, in these cases, is auto-assigned by XGBoost) and whether it's an evaluation metric, feature importance, or SHAP value. We also have `y/validation` for true labels from the validation set and `y_hat/validation` for predicted labels on the same validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensors()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tensor we can ask for which steps we have data - in this case, every 2 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(trial.tensor(\"validation-merror\").steps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain each tensor at each step as a `numpy` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trial.tensor(\"train-merror\").step(5).value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "We can also create a simple function that visualizes the training and validation errors\n",
    "as the training progresses.\n",
    "We expect each training errors to get smaller over time, as the system converges to a good solution.\n",
    "Now, remember that this is an interactive analysis - we are showing these tensors to give an idea of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define a function that, for the given tensor name, walks through all \n",
    "# the iterations for which we have data and fetches the value.\n",
    "# Returns the set of steps and the values\n",
    "def get_data(trial, tname):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\"train-merror\", \"validation-merror\"]\n",
    "for metric in metrics_to_plot:\n",
    "    steps, data = get_data(trial, metric)\n",
    "    plt.plot(steps, data, label=metric)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances\n",
    "\n",
    "We can also visualize the feature importances as determined by\n",
    "[xgboost.get_fscore()](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.get_fscore).\n",
    "Note that feature importances with zero values are not included here\n",
    "(which means that those features were not used in any split condisitons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_collections(trial, collection_name, ylabel=''):\n",
    "    \n",
    "    plt.figure(\n",
    "        num=1, figsize=(8, 8), dpi=80,\n",
    "        facecolor='w', edgecolor='k')\n",
    "\n",
    "    features = trial.collection(collection_name).get_tensor_names()\n",
    "\n",
    "    # to avoid cluttering, we will plot only one out of 20 features\n",
    "    for feature in list(features)[::20]:\n",
    "        steps, data = get_data(trial, feature)\n",
    "        label = feature.replace('/' + collection_name, '')\n",
    "        plt.plot(steps, data, label=label)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.04,1), loc='upper left')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collections(trial, \"feature_importance\", \"Feature importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "\n",
    "[SHAP](https://github.com/slundberg/shap) (SHapley Additive exPlanations) is\n",
    "another approach to explain the output of machine learning models.\n",
    "SHAP values represent a feature's contribution to a change in the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collections(trial, \"average_shap\", \"SHAP values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for step in range(0, 9):\n",
    "    cm = confusion_matrix(\n",
    "        trial.tensor('labels').step(step).value,\n",
    "        trial.tensor('predictions').step(step).value\n",
    "    )\n",
    "    normalized_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(normalized_cm, cmap=\"bone\", ax=ax, cbar=False, annot=cm, fmt='')\n",
    "    print(f\"iteartion: {step}\")\n",
    "    display(fig)\n",
    "    plt.pause(1)\n",
    "    ax.clear()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1P rule: Confusion matrix\n",
    "\n",
    "As another example of using a first party (1P) rule provided by Tornasole, let us again train the example training script [xgboost_mnist_basic_hook_demo.py](../scripts/xgboost_abalone_basic_hook_demo.py) and use a 1P rule `Confusion` to monitor the training job in realtime.\n",
    "\n",
    "During training, `Confusion` Rule job will monitor whether you are running into a situation where the ratio of on-diagonal and off-diagonal values in the confusion matrix is not within a specified range. In other words, this rule evaluates the goodness of a confusion matrix for a classification problem. It creates a matrix of size `category_no` $\\times$ `category_no` and populates it with data coming from (`y`, `y_hat`) pairs. For each (`y`, `y_hat`) pairs the count in `confusion[y][y_hat]` is  incremented by 1. Once the matrix is fully populated, the ratio of data on- and off-diagonal will be evaluated according to:\n",
    "\n",
    "- For elements on the diagonal:\n",
    "\n",
    "$$ \\frac{ \\text{confusion}_{ii} }{ \\sum_j \\text{confusion}_{jj} } \\geq \\text{min_diag} $$\n",
    "\n",
    "- For elements off the diagonal:\n",
    "\n",
    "$$ \\frac{ \\text{confusion}_{ji} }{ \\sum_j \\text{confusion}_{ji} } \\leq \\text{max_off_diag} $$\n",
    "\n",
    "If the condition is met, the rule will emit a cloudwatch event.\n",
    "\n",
    "Note that this rule will infer the default parameters if configurations are not specified, so you can simply use\n",
    "\n",
    "```python\n",
    "rules_specification = [\n",
    "    {\n",
    "        \"RuleName\": \"Confusion\",\n",
    "        \"InstanceType\": \"ml.c5.4xlarge\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "If you want to specify the optional parameters, you can do so by using `RuntimeConfigurations`:\n",
    "\n",
    "```python\n",
    "rules_specification = [\n",
    "    {\n",
    "        \"RuleName\": \"Confusion\",\n",
    "        \"InstanceType\": \"ml.c5.4xlarge\",\n",
    "        \"RuntimeConfigurations\": {\n",
    "            \"category_no\": \"10\",\n",
    "            \"min_diag\": \"0.8\",\n",
    "            \"max_diag\": \"0.2\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "For `Confusion` Rule API and other 1P rules that can be used in XGBoost, refer to [FirstPartyRules.md](../../../rules/FirstPartyRules.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBoost(\n",
    "    image_name=docker_image_name,\n",
    "    base_job_name=\"demo-tornasole-xgboost-confusion\",\n",
    "    entry_point=entry_point_script,\n",
    "    hyperparameters=hyperparameters,\n",
    "    train_instance_type=\"ml.m4.4xlarge\",\n",
    "    train_instance_count=1,\n",
    "    framework_version=\"0.90-1\",\n",
    "    py_version=\"py3\",\n",
    "    role=ROLE,\n",
    "\n",
    "    debug=True,\n",
    "    rules_specification=[\n",
    "        {\n",
    "        \"RuleName\": \"Confusion\",\n",
    "        \"InstanceType\": \"ml.c5.4xlarge\"\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(wait=False)\n",
    "\n",
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=job_name)\n",
    "\n",
    "description[\"TrainingJobStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description[\"DebugConfig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.describe_rule_execution_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showed two examples of using 1P rules provided Tornasole, but you can also write your own rules looking at these 1P rules for inspiration. Refer to [DeveloperGuide_Rules.md](../../../rules/DeveloperGuide_Rules.md) for more on the APIs you can use to write your own rules as well as descriptions for the 1P rules that we provide. [xgboost_regression.ipynb](xgboost_regression.ipynb) also demonstrates how to use a custom rule that monitors the ratio of feature importance values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
